#!/usr/bin/env python3
"""
Script de benchmark pour tester les diff√©rentes combinaisons RAG
"""

import sys
import csv
import json
import time
import shutil
import subprocess
import threading
import queue
from pathlib import Path
from datetime import datetime
import psutil

# Ajouter le dossier Client au path pour importer les modules
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT / "Client"))

# Importer les modules de recherche
import faq_search
import pls_search
import rag_assistant
from sentence_transformers import SentenceTransformer
from ollama_client import OllamaClient


class ResourceMonitor:
    """Monitore l'utilisation CPU, RAM et GPU avec macmon sur Apple Silicon"""

    def __init__(self, use_macmon=True):
        self.use_macmon = use_macmon
        self.monitoring = False
        self.monitor_thread = None
        self.reader_thread = None
        self.macmon_proc = None
        self.data_queue = queue.Queue()
        self.cpu_samples = []
        self.ram_samples = []
        self.gpu_samples = []

    def _read_macmon_output(self):
        """Thread s√©par√© pour lire la sortie macmon de mani√®re non-bloquante"""
        try:
            for line in self.macmon_proc.stdout:
                self.data_queue.put(line)
        except Exception:
            pass

    def start(self):
        """D√©marre le monitoring avec macmon ou psutil"""
        self.monitoring = True
        self.cpu_samples = []
        self.ram_samples = []
        self.gpu_samples = []

        # D√©marrer macmon seulement si demand√©
        if self.use_macmon and shutil.which("macmon"):
            try:
                self.macmon_proc = subprocess.Popen(
                    ["macmon", "pipe", "-i", "100"],  # 100ms interval (plus rapide)
                    stdout=subprocess.PIPE,
                    stderr=subprocess.DEVNULL,
                    text=True,
                    bufsize=1,  # Line buffered
                )
                # Thread d√©di√© pour lire macmon (non-bloquant)
                self.reader_thread = threading.Thread(target=self._read_macmon_output, daemon=True)
                self.reader_thread.start()
                # Attendre un peu que macmon d√©marre
                time.sleep(0.2)
            except Exception:
                self.macmon_proc = None

        self.monitor_thread = threading.Thread(target=self._monitor_loop, daemon=True)
        self.monitor_thread.start()

    def stop(self):
        """Arr√™te le monitoring et retourne les statistiques"""
        # Si on utilise macmon, attendre pour capturer des donn√©es
        if self.use_macmon and self.macmon_proc:
            time.sleep(0.5)

        self.monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join(timeout=1.0)

        if self.macmon_proc:
            self.macmon_proc.terminate()
            try:
                self.macmon_proc.wait(timeout=1.0)
            except subprocess.TimeoutExpired:
                self.macmon_proc.kill()

        stats = {
            'cpu_avg': None,
            'cpu_max': None,
            'ram_avg': None,
            'ram_max': None,
            'gpu_avg': None,
            'gpu_max': None
        }

        if self.cpu_samples:
            stats['cpu_avg'] = sum(self.cpu_samples) / len(self.cpu_samples)
            stats['cpu_max'] = max(self.cpu_samples)

        if self.ram_samples:
            stats['ram_avg'] = sum(self.ram_samples) / len(self.ram_samples)
            stats['ram_max'] = max(self.ram_samples)

        if self.gpu_samples:
            stats['gpu_avg'] = sum(self.gpu_samples) / len(self.gpu_samples)
            stats['gpu_max'] = max(self.gpu_samples)

        return stats

    def _parse_percent(self, value):
        """Convertir les valeurs macmon en pourcentage"""
        if value is None:
            return None
        if isinstance(value, (int, float)):
            return float(value * 100.0) if value <= 1 else float(value)
        return None

    def _monitor_loop(self):
        """Boucle de monitoring (s'ex√©cute dans un thread s√©par√©)"""
        if self.macmon_proc:
            while self.monitoring:
                try:
                    # Essayer de lire depuis la queue avec timeout
                    line = self.data_queue.get(timeout=0.1)

                    data = json.loads(line)

                    # CPU - format: [freq_mhz, usage_ratio]
                    cpu_source = data.get("pcpu_usage")
                    if cpu_source and isinstance(cpu_source, list) and len(cpu_source) >= 2:
                        cpu_ratio = cpu_source[1]
                        if isinstance(cpu_ratio, (int, float)):
                            cpu_pct = float(cpu_ratio * 100.0)
                            self.cpu_samples.append(cpu_pct)

                    # RAM - format: {"ram_usage": bytes, "ram_total": bytes}
                    mem_source = data.get("memory")
                    if mem_source and isinstance(mem_source, dict):
                        ram_usage = mem_source.get("ram_usage")
                        ram_total = mem_source.get("ram_total")
                        if ram_usage is not None and ram_total and ram_total > 0:
                            ram_pct = (ram_usage / ram_total) * 100.0
                            self.ram_samples.append(ram_pct)

                    # GPU - format: [freq_mhz, usage_ratio]
                    gpu_source = data.get("gpu_usage")
                    if gpu_source and isinstance(gpu_source, list) and len(gpu_source) >= 2:
                        gpu_ratio = gpu_source[1]
                        if isinstance(gpu_ratio, (int, float)):
                            gpu_pct = float(gpu_ratio * 100.0)
                            self.gpu_samples.append(gpu_pct)

                except queue.Empty:
                    # Pas de donn√©es dans la queue, continuer
                    continue
                except json.JSONDecodeError:
                    continue
        else:
            # Fallback: utiliser psutil uniquement
            while self.monitoring:
                try:
                    cpu_percent = psutil.cpu_percent(interval=0.5)
                    self.cpu_samples.append(cpu_percent)

                    ram = psutil.virtual_memory()
                    self.ram_samples.append(ram.percent)

                    time.sleep(0.5)
                except Exception:
                    pass


def load_questions(filepath, limit=None):
    """
    Charge les questions depuis un fichier texte

    Args:
        filepath: Chemin du fichier contenant les questions
        limit: Nombre maximum de questions √† charger (None = toutes)

    Returns:
        Liste de questions
    """
    questions = []

    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            # V√©rifier la limite
            if limit is not None and len(questions) >= limit:
                break

            line = line.strip()

            # Ignorer les lignes vides et les commentaires
            if not line or line.startswith('#'):
                continue

            # Enlever le num√©ro au d√©but (format: "1. **Titre:** Question")
            if '.' in line:
                parts = line.split('.', 1)
                if parts[0].strip().isdigit():
                    question = parts[1].strip()
                    # Enlever les ast√©risques
                    question = question.replace('**', '').replace('*', '')
                    questions.append(question)
                else:
                    questions.append(line)
            else:
                questions.append(line)

    return questions


def benchmark_search(opensearch_client, question, corpus, search_mode):
    """
    Effectue une recherche et mesure le temps de r√©ponse

    Args:
        opensearch_client: Client OpenSearch
        question: La question √† rechercher
        corpus: 'faq' ou 'pls'
        search_mode: 'keyword', 'semantic', 'neural', 'hybrid'

    Returns:
        dict: R√©sultats avec temps de r√©ponse
    """
    # D√©marrer le monitoring des ressources (psutil uniquement, plus rapide)
    monitor = ResourceMonitor(use_macmon=False)
    monitor.start()

    # D√©marrer le chronom√®tre
    start_time = time.time()
    start_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    result = {
        'question': question,
        'corpus': corpus,
        'search_mode': search_mode,
        'llm_model': '',
        'multiquery': '',
        'start_time': start_datetime,
        'end_time': None,
        'response_time': None,
        'num_results': 0,
        'cpu_avg': None,
        'cpu_max': None,
        'ram_avg': None,
        'ram_max': None,
        'gpu_avg': None,
        'gpu_max': None,
        'error': None
    }

    try:

        # Effectuer la recherche selon le corpus et le mode
        if corpus == 'faq':
            if search_mode == 'keyword':
                index_name = faq_search.FAQ_INDEX_NAME
                response = faq_search.search_faq_by_keyword(
                    opensearch_client,
                    index_name,
                    question,
                    size=5
                )
            elif search_mode == 'semantic':
                # Charger le mod√®le pour la recherche s√©mantique
                model = SentenceTransformer(faq_search.EMBEDDING_MODEL)
                response = faq_search.search_faq_semantic(
                    opensearch_client,
                    model,
                    question,
                    size=5
                )
            elif search_mode == 'neural':
                response = faq_search.search_faq_neural(
                    opensearch_client,
                    faq_search.ML_MODEL_ID,
                    question,
                    size=5
                )
            elif search_mode == 'hybrid':
                response = faq_search.search_faq_hybrid(
                    opensearch_client,
                    faq_search.ML_MODEL_ID,
                    question,
                    size=5
                )
            else:
                raise ValueError(f"Mode de recherche inconnu: {search_mode}")
        elif corpus == 'pls':
            if search_mode == 'keyword':
                index_name = pls_search.PLS_INDEX_NAME
                response = pls_search.search_pls_by_keyword(
                    opensearch_client,
                    index_name,
                    question,
                    size=5
                )
            elif search_mode == 'semantic':
                # Charger le mod√®le pour la recherche s√©mantique
                model = SentenceTransformer(pls_search.EMBEDDING_MODEL)
                response = pls_search.search_pls_semantic(
                    opensearch_client,
                    model,
                    question,
                    size=5
                )
            elif search_mode == 'neural':
                response = pls_search.search_pls_neural(
                    opensearch_client,
                    pls_search.ML_MODEL_ID,
                    question,
                    size=5
                )
            elif search_mode == 'hybrid':
                response = pls_search.search_pls_hybrid(
                    opensearch_client,
                    pls_search.ML_MODEL_ID,
                    question,
                    size=5
                )
            else:
                raise ValueError(f"Mode de recherche inconnu: {search_mode}")
        else:
            raise NotImplementedError(f"Corpus {corpus} non impl√©ment√©")

        # Mesurer le temps
        result['response_time'] = time.time() - start_time

        # R√©cup√©rer les r√©sultats
        hits = response["hits"]["hits"]
        result['num_results'] = len(hits)

    except Exception as e:
        result['error'] = str(e)

    # Arr√™ter le monitoring et r√©cup√©rer les statistiques
    stats = monitor.stop()
    result.update(stats)

    # Enregistrer l'heure de fin
    result['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    return result


def benchmark_rag(opensearch_client, ollama_client, question, corpus, search_mode, llm_model, multiquery_enabled):
    """
    Effectue un benchmark RAG complet (recherche + g√©n√©ration)

    Args:
        opensearch_client: Client OpenSearch
        ollama_client: Client Ollama
        question: La question √† poser
        corpus: 'faq' ou 'pls'
        search_mode: 'keyword', 'semantic', 'neural', 'hybrid'
        llm_model: Nom du mod√®le LLM √† utiliser
        multiquery_enabled: True pour activer le multi-query

    Returns:
        dict: R√©sultats avec temps de r√©ponse
    """
    # D√©marrer le monitoring des ressources
    monitor = ResourceMonitor()
    monitor.start()

    # D√©marrer le chronom√®tre global
    start_time = time.time()
    start_datetime = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    result = {
        'question': question,
        'corpus': corpus,
        'search_mode': search_mode,
        'llm_model': llm_model,
        'multiquery': multiquery_enabled,
        'start_time': start_datetime,
        'end_time': None,
        'response_time': None,
        'search_time': None,
        'generation_time': None,
        'num_results': 0,
        'cpu_avg': None,
        'cpu_max': None,
        'ram_avg': None,
        'ram_max': None,
        'gpu_avg': None,
        'gpu_max': None,
        'error': None
    }

    try:

        # Charger le mod√®le d'embedding si n√©cessaire
        embedding_model = None
        if search_mode == 'semantic':
            embedding_model = SentenceTransformer(faq_search.EMBEDDING_MODEL)

        # Configurer le mod√®le LLM
        ollama_client.model = llm_model

        # Temps de recherche
        search_start = time.time()

        if multiquery_enabled:
            # Mode Multi-Query: g√©n√©rer 3 questions et chercher 2 r√©sultats par question
            alternative_questions = rag_assistant.generate_alternative_questions(ollama_client, question)

            # Chercher avec chaque question (2 r√©sultats par question)
            all_hits = []
            for alt_question in alternative_questions:
                response = rag_assistant.perform_search(
                    opensearch_client,
                    embedding_model,
                    corpus,
                    search_mode,
                    alt_question,
                    num_results=2
                )
                hits = response["hits"]["hits"]
                all_hits.extend(hits)

            result['num_results'] = len(all_hits)

            # Formater le contexte √† partir de tous les r√©sultats
            context_parts = []
            for i, hit in enumerate(all_hits, 1):
                source = hit["_source"]
                score = hit["_score"]

                if corpus == 'faq':
                    context_parts.append(
                        f"[Document {i} - Pertinence: {score:.2f}]\n"
                        f"Question: {source['question']}\n"
                        f"R√©ponse: {source['answer']}\n"
                    )
                else:
                    title = source.get('title', '')
                    title_str = f"Titre: {title}\n" if title else ""
                    context_parts.append(
                        f"[Document {i} - Pertinence: {score:.2f}]\n"
                        f"Source: {source['filename']} (Page {source['page']})\n"
                        f"{title_str}"
                        f"Contenu: {source['text']}\n"
                    )

            context = "\n".join(context_parts) if context_parts else "Aucun r√©sultat trouv√©."

        else:
            # Mode simple: recherche classique
            response = rag_assistant.perform_search(
                opensearch_client,
                embedding_model,
                corpus,
                search_mode,
                question,
                num_results=5
            )

            hits = response["hits"]["hits"]
            result['num_results'] = len(hits)

            # Formater le contexte
            if corpus == 'faq':
                context = rag_assistant.format_faq_results_as_context(response)
            else:
                context = rag_assistant.format_pls_results_as_context(response)

        result['search_time'] = time.time() - search_start

        # Temps de g√©n√©ration
        generation_start = time.time()

        # G√©n√©rer la r√©ponse avec le LLM (sans streaming et sans affichage pour le benchmark)
        llm_response = rag_assistant.generate_rag_answer(ollama_client, question, context, stream=False, display=False)
        result['llm_response'] = llm_response

        result['generation_time'] = time.time() - generation_start

        # Mesurer le temps total
        result['response_time'] = time.time() - start_time

    except Exception as e:
        result['error'] = str(e)

    # Arr√™ter le monitoring et r√©cup√©rer les statistiques
    stats = monitor.stop()
    result.update(stats)

    # Enregistrer l'heure de fin
    result['end_time'] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    return result


def save_results_to_csv(results, output_file):
    """
    Sauvegarde les r√©sultats dans un fichier CSV

    Args:
        results: Liste de dictionnaires de r√©sultats
        output_file: Chemin du fichier CSV
    """
    if not results:
        print("Aucun r√©sultat √† sauvegarder")
        return

    fieldnames = ['question', 'corpus', 'search_mode', 'llm_model', 'multiquery',
                  'start_time', 'end_time', 'response_time', 'num_results',
                  'cpu_avg', 'cpu_max', 'ram_avg', 'ram_max', 'gpu_avg', 'gpu_max',
                  'error']

    with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, extrasaction='ignore')
        writer.writeheader()
        writer.writerows(results)

    print(f"‚úì R√©sultats sauvegard√©s dans: {output_file}")


def main():
    """Fonction principale"""
    print("=" * 70)
    print("=== Benchmark RAG ===")
    print("=" * 70)

    # Connexion √† OpenSearch
    print("\n[1/2] Connexion √† OpenSearch...")
    try:
        opensearch_client = faq_search.create_opensearch_client()
        info = opensearch_client.info()
        print(f"‚úì Connect√© √† OpenSearch version {info['version']['number']}")
    except Exception as e:
        print(f"‚úó Erreur de connexion : {e}")
        return

    # Dossier contenant les questions
    benchmark_dir = Path(__file__).parent

    # Limite de questions √† charger (pour les tests rapides)
    QUESTION_LIMIT = 30

    # Charger les questions FAQ
    print("\n[2/2] Chargement des questions...")
    faq_file = benchmark_dir / "faq_question.txt"
    faq_questions = []

    if faq_file.exists():
        faq_questions = load_questions(faq_file, limit=QUESTION_LIMIT)
        print(f"‚úì {len(faq_questions)} questions FAQ charg√©es (limite: {QUESTION_LIMIT})")

        # Afficher les premi√®res questions
        print("\nExemples de questions FAQ:")
        for i, q in enumerate(faq_questions[:3], 1):
            print(f"  {i}. {q[:80]}...")
    else:
        print("‚ö†Ô∏è  Fichier faq_question.txt non trouv√©")

    # Charger les questions Pour La Science
    pls_file = benchmark_dir / "pls_question.txt"
    pls_questions = []

    if pls_file.exists():
        pls_questions = load_questions(pls_file, limit=QUESTION_LIMIT)
        print(f"‚úì {len(pls_questions)} questions Pour La Science charg√©es (limite: {QUESTION_LIMIT})")

        # Afficher les premi√®res questions
        print("\nExemples de questions PLS:")
        for i, q in enumerate(pls_questions[:3], 1):
            print(f"  {i}. {q[:80]}...")
    else:
        print("‚ö†Ô∏è  Fichier pls_question.txt non trouv√©")

    # Modes de recherche √† tester
    search_modes = ['keyword', 'semantic', 'neural', 'hybrid']

    # Cr√©er le dossier resultats s'il n'existe pas
    results_dir = benchmark_dir / "resultats"
    results_dir.mkdir(exist_ok=True)
    print(f"\n‚úì Dossier de r√©sultats: {results_dir}")

    # Ex√©cuter le benchmark pour chaque mode de recherche
    for search_mode in search_modes:
        # Benchmark FAQ pour ce mode
        if faq_questions:
            print("\n" + "=" * 70)
            print(f"Benchmark FAQ - Mode {search_mode.upper()}")
            print("=" * 70)

            results = []
            total = len(faq_questions)

            for i, question in enumerate(faq_questions, 1):
                print(f"\n[{i}/{total}] Question: {question[:60]}...")

                result = benchmark_search(
                    opensearch_client,
                    question,
                    corpus='faq',
                    search_mode=search_mode
                )

                results.append(result)

                if result['error']:
                    print(f"  ‚úó Erreur: {result['error']}")
                else:
                    print(f"  ‚úì Temps: {result['response_time']:.3f}s | R√©sultats: {result['num_results']}")

            # Sauvegarder les r√©sultats
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = results_dir / f"benchmark_faq_{search_mode}_{timestamp}.csv"
            save_results_to_csv(results, output_file)

            # Statistiques
            successful_results = [r for r in results if r['error'] is None]
            if successful_results:
                avg_time = sum(r['response_time'] for r in successful_results) / len(successful_results)
                print(f"\nüìä Statistiques:")
                print(f"  - Questions trait√©es: {len(successful_results)}/{total}")
                print(f"  - Temps moyen: {avg_time:.3f}s")

            print(f"\n‚è∏Ô∏è  Pause de 5 minutes avant la prochaine √©tape...")
            time.sleep(600)

        # Benchmark PLS pour ce mode
        if pls_questions:
            print("\n" + "=" * 70)
            print(f"Benchmark PLS - Mode {search_mode.upper()}")
            print("=" * 70)

            results = []
            total = len(pls_questions)

            for i, question in enumerate(pls_questions, 1):
                print(f"\n[{i}/{total}] Question: {question[:60]}...")

                result = benchmark_search(
                    opensearch_client,
                    question,
                    corpus='pls',
                    search_mode=search_mode
                )

                results.append(result)

                if result['error']:
                    print(f"  ‚úó Erreur: {result['error']}")
                else:
                    print(f"  ‚úì Temps: {result['response_time']:.3f}s | R√©sultats: {result['num_results']}")

            # Sauvegarder les r√©sultats
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = results_dir / f"benchmark_pls_{search_mode}_{timestamp}.csv"
            save_results_to_csv(results, output_file)

            # Statistiques
            successful_results = [r for r in results if r['error'] is None]
            if successful_results:
                avg_time = sum(r['response_time'] for r in successful_results) / len(successful_results)
                print(f"\nüìä Statistiques:")
                print(f"  - Questions trait√©es: {len(successful_results)}/{total}")
                print(f"  - Temps moyen: {avg_time:.3f}s")

            print(f"\n‚è∏Ô∏è  Pause de 5 minutes avant la prochaine √©tape...")
            time.sleep(600)

    # R√©sum√©
    print("\n" + "=" * 70)
    print(f"Total: {len(faq_questions) + len(pls_questions)} questions charg√©es")
    print("=" * 70)

    # ========================================================================
    # PARTIE RAG - Benchmark avec LLM
    # ========================================================================

    print("\n" + "=" * 70)
    print("=== Benchmark RAG (Recherche + G√©n√©ration) ===")
    print("=" * 70)

    # Connexion √† Ollama
    print("\nConnexion √† Ollama...")
    ollama_client = OllamaClient()

    if not ollama_client.check_connection():
        print("‚ö†Ô∏è  Impossible de se connecter √† Ollama - Benchmark RAG ignor√©")
        print("üí° Assurez-vous qu'Ollama est lanc√© : ollama serve")
    else:
        print("‚úì Connect√© √† Ollama")

        # Mod√®les LLM √† tester
        llm_models = ['gpt-oss:20b', 'llama3.2']

        # Modes multi-query √† tester
        multiquery_modes = [False, True]

        # Ex√©cuter le benchmark RAG pour chaque combinaison
        for search_mode in search_modes:
            for llm_model in llm_models:
                for multiquery_enabled in multiquery_modes:
                    multiquery_str = "multi-query" if multiquery_enabled else "simple"

                    # Benchmark RAG FAQ pour cette combinaison
                    if faq_questions:
                        print("\n" + "=" * 70)
                        print(f"Benchmark RAG FAQ - {search_mode.upper()} + {llm_model} ({multiquery_str})")
                        print("=" * 70)

                        results = []
                        total = len(faq_questions)

                        for i, question in enumerate(faq_questions, 1):
                            print(f"\n[{i}/{total}] Question: {question[:60]}...")

                            result = benchmark_rag(
                                opensearch_client,
                                ollama_client,
                                question,
                                corpus='faq',
                                search_mode=search_mode,
                                llm_model=llm_model,
                                multiquery_enabled=multiquery_enabled
                            )

                            results.append(result)

                            if result['error']:
                                print(f"  ‚úó Erreur: {result['error']}")
                            else:
                                print(f"  ‚úì Temps: {result['response_time']:.3f}s | R√©sultats: {result['num_results']}")
                                # Afficher les 100 premiers caract√®res de la r√©ponse
                                if 'llm_response' in result and result['llm_response']:
                                    response_preview = result['llm_response'][:100].replace('\n', ' ')
                                    print(f"  üìù R√©ponse: {response_preview}...")

                        # Sauvegarder les r√©sultats
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"benchmark_rag_faq_{search_mode}_{llm_model}_{multiquery_str}_{timestamp}.csv"
                        output_file = results_dir / filename
                        save_results_to_csv(results, output_file)

                        # Statistiques
                        successful_results = [r for r in results if r['error'] is None]
                        if successful_results:
                            avg_time = sum(r['response_time'] for r in successful_results) / len(successful_results)
                            print(f"\nüìä Statistiques:")
                            print(f"  - Questions trait√©es: {len(successful_results)}/{total}")
                            print(f"  - Temps moyen: {avg_time:.3f}s")

                        print(f"\n‚è∏Ô∏è  Pause de 5 minutes avant la prochaine √©tape...")
                        time.sleep(600)

                    # Benchmark RAG PLS pour cette combinaison
                    if pls_questions:
                        print("\n" + "=" * 70)
                        print(f"Benchmark RAG PLS - {search_mode.upper()} + {llm_model} ({multiquery_str})")
                        print("=" * 70)

                        results = []
                        total = len(pls_questions)

                        for i, question in enumerate(pls_questions, 1):
                            print(f"\n[{i}/{total}] Question: {question[:60]}...")

                            result = benchmark_rag(
                                opensearch_client,
                                ollama_client,
                                question,
                                corpus='pour_la_science',
                                search_mode=search_mode,
                                llm_model=llm_model,
                                multiquery_enabled=multiquery_enabled
                            )

                            results.append(result)

                            if result['error']:
                                print(f"  ‚úó Erreur: {result['error']}")
                            else:
                                print(f"  ‚úì Temps: {result['response_time']:.3f}s | R√©sultats: {result['num_results']}")
                                # Afficher les 100 premiers caract√®res de la r√©ponse
                                if 'llm_response' in result and result['llm_response']:
                                    response_preview = result['llm_response'][:100].replace('\n', ' ')
                                    print(f"  üìù R√©ponse: {response_preview}...")

                        # Sauvegarder les r√©sultats
                        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                        filename = f"benchmark_rag_pls_{search_mode}_{llm_model}_{multiquery_str}_{timestamp}.csv"
                        output_file = results_dir / filename
                        save_results_to_csv(results, output_file)

                        # Statistiques
                        successful_results = [r for r in results if r['error'] is None]
                        if successful_results:
                            avg_time = sum(r['response_time'] for r in successful_results) / len(successful_results)
                            print(f"\nüìä Statistiques:")
                            print(f"  - Questions trait√©es: {len(successful_results)}/{total}")
                            print(f"  - Temps moyen: {avg_time:.3f}s")

                        # Pause de 3 minutes avant la prochaine √©tape (sauf si c'est la derni√®re)
                        is_last = (search_mode == search_modes[-1] and
                                   llm_model == llm_models[-1] and
                                   multiquery_enabled == multiquery_modes[-1])
                        if not is_last:
                            print(f"\n‚è∏Ô∏è  Pause de 5 minutes avant la prochaine √©tape...")
                            time.sleep(600)

    print("\n" + "=" * 70)
    print("=== Benchmark termin√© ===")
    print("=" * 70)


if __name__ == "__main__":
    main()
