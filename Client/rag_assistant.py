#!/usr/bin/env python3
"""
Assistant RAG - Combine recherche OpenSearch et g√©n√©ration avec Ollama
Utilise les modules faq_search, pls_search et ollama_client
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# Ajouter le dossier Client au path pour les imports
sys.path.insert(0, str(Path(__file__).parent))

# Importer les modules de recherche
import faq_search
import pls_search
from ollama_client import OllamaClient

# Charger les variables d'environnement
PROJECT_ROOT = Path(__file__).parent.parent
env_path = PROJECT_ROOT / '.env'
load_dotenv(env_path)

# Configuration
EMBEDDING_MODEL = os.environ['EMBEDDING_MODEL']
ML_MODEL_ID = os.environ.get('MODEL_ID', '')
OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', 'llama3.2')


# ============================================================================
# FORMATAGE DES R√âSULTATS
# ============================================================================

def format_faq_results_as_context(response):
    """Formate les r√©sultats FAQ en contexte pour le LLM"""
    hits = response["hits"]["hits"]

    if not hits:
        return "Aucun r√©sultat trouv√© dans la FAQ."

    context_parts = []
    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        context_parts.append(
            f"[Document {i} - Pertinence: {score:.2f}]\n"
            f"Question: {source['question']}\n"
            f"R√©ponse: {source['answer']}\n"
        )

    return "\n".join(context_parts)


def format_pls_results_as_context(response):
    """Formate les r√©sultats Pour La Science en contexte pour le LLM"""
    hits = response["hits"]["hits"]

    if not hits:
        return "Aucun r√©sultat trouv√© dans Pour La Science."

    context_parts = []
    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        title = source.get('title', '')
        title_str = f"Titre: {title}\n" if title else ""

        context_parts.append(
            f"[Document {i} - Pertinence: {score:.2f}]\n"
            f"Source: {source['filename']} (Page {source['page']})\n"
            f"{title_str}"
            f"Contenu: {source['text']}\n"
        )

    return "\n".join(context_parts)


def display_faq_results(response):
    """Affiche les r√©sultats FAQ de mani√®re lisible"""
    hits = response["hits"]["hits"]
    total = response["hits"]["total"]["value"]

    print(f"\n{'=' * 70}")
    print(f"üìö R√©sultats FAQ : {total} documents trouv√©s")
    print(f"{'=' * 70}\n")

    if not hits:
        print("Aucun r√©sultat trouv√©.")
        return

    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        print(f"--- Document {i} (score: {score:.4f}) ---")
        print(f"Q: {source['question']}")
        answer = source['answer']
        if len(answer) > 150:
            answer = answer[:150] + "..."
        print(f"R: {answer}")
        if source.get('tags'):
            print(f"Tags: {', '.join(source['tags'])}")
        print()


def display_pls_results(response):
    """Affiche les r√©sultats Pour La Science de mani√®re lisible"""
    hits = response["hits"]["hits"]
    total = response["hits"]["total"]["value"]

    print(f"\n{'=' * 70}")
    print(f"üì∞ R√©sultats Pour La Science : {total} documents trouv√©s")
    print(f"{'=' * 70}\n")

    if not hits:
        print("Aucun r√©sultat trouv√©.")
        return

    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        print(f"--- Document {i} (score: {score:.4f}) ---")
        print(f"Fichier: {source['filename']} - Page {source['page']}")

        if source.get('title'):
            print(f"Titre: {source['title']}")

        text = source['text']
        if len(text) > 150:
            text = text[:150] + "..."
        print(f"Texte: {text}")
        print()


# ============================================================================
# INTERFACE UTILISATEUR
# ============================================================================

def select_corpus():
    """S√©lection du corpus de recherche"""
    print("\nChoisissez le corpus de recherche :")
    print("-" * 70)
    print("1. FAQ CielNet")
    print("2. Pour La Science")
    print("-" * 70)

    while True:
        choice = input("\nVotre choix (1-2) : ").strip()
        if choice == '1':
            print("‚úì Corpus s√©lectionn√© : FAQ CielNet")
            return 'faq'
        elif choice == '2':
            print("‚úì Corpus s√©lectionn√© : Pour La Science")
            return 'pour_la_science'
        else:
            print("Choix invalide. Veuillez entrer 1 ou 2.")


def select_search_mode():
    """S√©lection du mode de recherche"""
    print("\nChoisissez le mode de recherche :")
    print("-" * 70)
    print("1. Mot-cl√© (BM25)")
    print("2. S√©mantique (KNN avec embeddings)")
    print("3. Neural (embeddings OpenSearch)")
    print("4. Hybride (BM25 + Neural)")
    print("-" * 70)

    while True:
        choice = input("\nVotre choix (1-4) : ").strip()
        if choice == '1':
            print("‚úì Mode : Recherche par mot-cl√©")
            return 'keyword'
        elif choice == '2':
            print("‚úì Mode : Recherche s√©mantique")
            return 'semantic'
        elif choice == '3':
            if not ML_MODEL_ID:
                print("‚ö†Ô∏è  MODEL_ID non configur√© - Mode mot-cl√© utilis√© par d√©faut")
                return 'keyword'
            print("‚úì Mode : Recherche neural")
            return 'neural'
        elif choice == '4':
            if not ML_MODEL_ID:
                print("‚ö†Ô∏è  MODEL_ID non configur√© - Mode mot-cl√© utilis√© par d√©faut")
                return 'keyword'
            print("‚úì Mode : Recherche hybride")
            return 'hybrid'
        else:
            print("Choix invalide. Veuillez entrer 1, 2, 3 ou 4.")


def select_llm_model(ollama_client):
    """S√©lection du mod√®le LLM"""
    models = ollama_client.list_models()

    if not models:
        print("‚ö†Ô∏è  Aucun mod√®le Ollama trouv√©, utilisation du mod√®le par d√©faut")
        return ollama_client.model

    model_names = [m.get('name') for m in models]

    print("\nMod√®les Ollama disponibles :")
    print("-" * 70)

    for i, model_name in enumerate(model_names, 1):
        marker = " (actuel)" if model_name == ollama_client.model else ""
        print(f"{i}. {model_name}{marker}")

    print("-" * 70)

    while True:
        choice = input(f"\nVotre choix (1-{len(model_names)}) ou Entr√©e pour garder actuel : ").strip()

        if not choice:
            print(f"‚úì Mod√®le s√©lectionn√© : {ollama_client.model}")
            return ollama_client.model

        if choice.isdigit() and 1 <= int(choice) <= len(model_names):
            selected = model_names[int(choice) - 1]
            print(f"‚úì Mod√®le s√©lectionn√© : {selected}")
            return selected
        else:
            print(f"Choix invalide. Veuillez entrer un nombre entre 1 et {len(model_names)}.")


def select_multiquery_mode():
    """S√©lection du mode multi-query"""
    print("\nActiver le mode Multi-Query ?")
    print("-" * 70)
    print("Le LLM g√©n√®rera 3 questions alternatives pour enrichir la recherche")
    print("(2 r√©sultats par question = max 6 documents)")
    print("-" * 70)
    print("1. Non (recherche simple avec 5 r√©sultats)")
    print("2. Oui (recherche multi-query avec 6 r√©sultats)")
    print("-" * 70)

    while True:
        choice = input("\nVotre choix (1-2) : ").strip()
        if choice == '1':
            print("‚úì Mode : Recherche simple")
            return False
        elif choice == '2':
            print("‚úì Mode : Multi-Query activ√©")
            return True
        else:
            print("Choix invalide. Veuillez entrer 1 ou 2.")


def perform_search(opensearch_client, embedding_model, corpus_type, search_mode, question, num_results=5):
    """Effectue la recherche selon le corpus et le mode s√©lectionn√©s"""

    if corpus_type == 'faq':
        # D√©terminer l'index
        if search_mode in ['neural', 'hybrid']:
            index_name = faq_search.FAQ_INDEX_NAME_PIPELINE
        elif search_mode == 'semantic':
            index_name = faq_search.FAQ_INDEX_NAME_SEMANTIC
        else:
            index_name = faq_search.FAQ_INDEX_NAME

        # Effectuer la recherche
        if search_mode == 'keyword':
            return faq_search.search_faq_by_keyword(opensearch_client, index_name, question, num_results)
        elif search_mode == 'semantic':
            return faq_search.search_faq_semantic(opensearch_client, embedding_model, question, num_results)
        elif search_mode == 'neural':
            return faq_search.search_faq_neural(opensearch_client, ML_MODEL_ID, question, num_results)
        elif search_mode == 'hybrid':
            return faq_search.search_faq_hybrid(opensearch_client, ML_MODEL_ID, question, num_results)

    else:  # pour_la_science
        # D√©terminer l'index
        if search_mode in ['neural', 'hybrid']:
            index_name = pls_search.PLS_INDEX_NAME_PIPELINE
        elif search_mode == 'semantic':
            index_name = pls_search.PLS_INDEX_NAME_SEMANTIC
        else:
            index_name = pls_search.PLS_INDEX_NAME

        # Effectuer la recherche
        if search_mode == 'keyword':
            return pls_search.search_pls_by_keyword(opensearch_client, index_name, question, num_results)
        elif search_mode == 'semantic':
            return pls_search.search_pls_semantic(opensearch_client, embedding_model, question, num_results)
        elif search_mode == 'neural':
            return pls_search.search_pls_neural(opensearch_client, ML_MODEL_ID, question, num_results)
        elif search_mode == 'hybrid':
            return pls_search.search_pls_hybrid(opensearch_client, ML_MODEL_ID, question, num_results)


def generate_alternative_questions(ollama_client, original_question):
    """G√©n√®re 3 questions alternatives pour am√©liorer la recherche"""
    prompt = f"""Tu es un assistant sp√©cialis√© dans la reformulation de questions pour am√©liorer les recherches documentaires.

QUESTION ORIGINALE: {original_question}

T√ÇCHE: G√©n√®re exactement 3 questions alternatives ou compl√©mentaires qui permettraient de trouver des informations pertinentes pour r√©pondre √† la question originale.

CONSIGNES:
- Les questions doivent aborder diff√©rents aspects ou angles de la question originale
- Sois pr√©cis et concis
- Utilise des termes et formulations vari√©s
- Format: Une question par ligne, num√©rot√©e 1., 2., 3.

QUESTIONS ALTERNATIVES:"""

    print(f"\nüîÑ G√©n√©ration de questions alternatives...")

    full_response = ""
    for chunk in ollama_client.generate(prompt, stream=False):
        full_response += chunk

    # Extraire les 3 questions
    questions = []
    for line in full_response.strip().split('\n'):
        line = line.strip()
        # Enlever les num√©ros et puces
        for prefix in ['1.', '2.', '3.', '1)', '2)', '3)', '-', '*']:
            if line.startswith(prefix):
                line = line[len(prefix):].strip()
                break
        if line and len(questions) < 3:
            questions.append(line)

    return questions[:3]  # S'assurer d'avoir exactement 3 questions max


def generate_rag_answer(ollama_client, question, context):
    """G√©n√®re une r√©ponse RAG avec Ollama"""
    prompt = f"""Tu es un assistant qui r√©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.

CONTEXTE DOCUMENTAIRE:
{context}

QUESTION: {question}

INSTRUCTIONS:
- R√©ponds √† la question en te basant uniquement sur le contexte fourni
- Si le contexte ne contient pas d'information pertinente pour r√©pondre, dis-le clairement
- Sois pr√©cis, concis et factuel
- Cite les sources quand c'est pertinent (num√©ro de document, page, etc.)

R√âPONSE:"""

    print(f"\n{'=' * 70}")
    print(f"ü§ñ R√©ponse de {ollama_client.model} :")
    print(f"{'=' * 70}\n")

    full_response = ""
    for chunk in ollama_client.generate(prompt, stream=True):
        print(chunk, end='', flush=True)
        full_response += chunk

    print("\n")
    return full_response


# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """Fonction principale"""
    print("=" * 70)
    print("=== üöÄ Assistant RAG - Recherche Augment√©e par G√©n√©ration ===")
    print("=" * 70)

    # [1/4] Connexion √† OpenSearch
    print("\n[1/4] Connexion √† OpenSearch...")
    try:
        opensearch_client = faq_search.create_opensearch_client()
        info = opensearch_client.info()
        print(f"‚úì Connect√© √† OpenSearch version {info['version']['number']}")
    except Exception as e:
        print(f"‚úó Erreur de connexion √† OpenSearch : {e}")
        return

    # [2/4] Connexion √† Ollama
    print("\n[2/4] Connexion √† Ollama...")
    ollama_client = OllamaClient()

    if not ollama_client.check_connection():
        print("‚úó Impossible de se connecter √† Ollama")
        print("üí° Assurez-vous qu'Ollama est lanc√© : ollama serve")
        return

    print(f"‚úì Connect√© √† Ollama")

    # [3/4] Configuration
    print("\n[3/4] Configuration")
    corpus_type = select_corpus()
    search_mode = select_search_mode()
    llm_model = select_llm_model(ollama_client)
    ollama_client.model = llm_model
    multiquery_enabled = select_multiquery_mode()

    # Charger le mod√®le d'embedding si n√©cessaire
    embedding_model = None
    if search_mode == 'semantic':
        print(f"\n‚è≥ Chargement du mod√®le d'embedding {EMBEDDING_MODEL}...")
        try:
            embedding_model = SentenceTransformer(EMBEDDING_MODEL)
            print("‚úì Mod√®le d'embedding charg√©")
        except Exception as e:
            print(f"‚úó Erreur lors du chargement : {e}")
            return

    # [4/4] Interface de questions-r√©ponses
    print("\n[4/4] Assistant RAG pr√™t")
    print("\n" + "=" * 70)
    print("üí¨ Posez vos questions !")
    print("\nCommandes disponibles :")
    print("  - Tapez votre question pour obtenir une r√©ponse")
    print("  - '/config' pour changer la configuration")
    print("  - '/exit' pour quitter")
    print("-" * 70)

    while True:
        question = input("\n‚ùì Question > ").strip()

        if not question:
            continue

        if question.lower() in ['/exit', '/quit', '/q']:
            print("\nüëã Au revoir!")
            break

        if question.lower() == '/config':
            print("\n" + "=" * 70)
            print("Reconfiguration")
            print("=" * 70)

            corpus_type = select_corpus()
            search_mode = select_search_mode()
            llm_model = select_llm_model(ollama_client)
            ollama_client.model = llm_model
            multiquery_enabled = select_multiquery_mode()

            # Recharger l'embedding model si n√©cessaire
            if search_mode == 'semantic' and embedding_model is None:
                print(f"\n‚è≥ Chargement du mod√®le d'embedding {EMBEDDING_MODEL}...")
                try:
                    embedding_model = SentenceTransformer(EMBEDDING_MODEL)
                    print("‚úì Mod√®le d'embedding charg√©")
                except Exception as e:
                    print(f"‚úó Erreur : {e}")
                    search_mode = 'keyword'
                    print("‚ö†Ô∏è  Retour au mode mot-cl√©")

            continue

        # Effectuer la recherche
        try:
            if multiquery_enabled:
                # Mode Multi-Query: g√©n√©rer 3 questions et chercher 2 r√©sultats par question
                alternative_questions = generate_alternative_questions(ollama_client, question)

                print(f"\nüìã Questions g√©n√©r√©es:")
                for i, q in enumerate(alternative_questions, 1):
                    print(f"  {i}. {q}")

                # Chercher avec chaque question (2 r√©sultats par question)
                all_hits = []
                doc_counter = 1

                for i, alt_question in enumerate(alternative_questions, 1):
                    print(f"\nüîç Recherche {i}/3 ({search_mode})...")
                    response = perform_search(
                        opensearch_client,
                        embedding_model,
                        corpus_type,
                        search_mode,
                        alt_question,
                        num_results=2
                    )

                    # Collecter les r√©sultats
                    hits = response["hits"]["hits"]
                    for hit in hits:
                        all_hits.append((doc_counter, hit))
                        doc_counter += 1

                # Afficher tous les r√©sultats collect√©s
                print(f"\n{'=' * 70}")
                print(f"üìö Total: {len(all_hits)} documents collect√©s")
                print(f"{'=' * 70}\n")

                for doc_num, hit in all_hits:
                    source = hit["_source"]
                    score = hit["_score"]

                    print(f"--- Document {doc_num} (score: {score:.4f}) ---")

                    if corpus_type == 'faq':
                        print(f"Q: {source['question']}")
                        answer = source['answer']
                        if len(answer) > 150:
                            answer = answer[:150] + "..."
                        print(f"R: {answer}")
                        if source.get('tags'):
                            print(f"Tags: {', '.join(source['tags'])}")
                    else:
                        print(f"Fichier: {source['filename']} - Page {source['page']}")
                        if source.get('title'):
                            print(f"Titre: {source['title']}")
                        text = source['text']
                        if len(text) > 150:
                            text = text[:150] + "..."
                        print(f"Texte: {text}")

                    print()

                # Formater le contexte √† partir de tous les r√©sultats
                context_parts = []
                for doc_num, hit in all_hits:
                    source = hit["_source"]
                    score = hit["_score"]

                    if corpus_type == 'faq':
                        context_parts.append(
                            f"[Document {doc_num} - Pertinence: {score:.2f}]\n"
                            f"Question: {source['question']}\n"
                            f"R√©ponse: {source['answer']}\n"
                        )
                    else:
                        title = source.get('title', '')
                        title_str = f"Titre: {title}\n" if title else ""
                        context_parts.append(
                            f"[Document {doc_num} - Pertinence: {score:.2f}]\n"
                            f"Source: {source['filename']} (Page {source['page']})\n"
                            f"{title_str}"
                            f"Contenu: {source['text']}\n"
                        )

                context = "\n".join(context_parts) if context_parts else "Aucun r√©sultat trouv√©."

            else:
                # Mode simple: recherche classique
                print(f"\nüîç Recherche en cours ({search_mode})...")
                response = perform_search(
                    opensearch_client,
                    embedding_model,
                    corpus_type,
                    search_mode,
                    question
                )

                # Afficher les r√©sultats de recherche
                if corpus_type == 'faq':
                    display_faq_results(response)
                    context = format_faq_results_as_context(response)
                else:
                    display_pls_results(response)
                    context = format_pls_results_as_context(response)

            # G√©n√©rer la r√©ponse avec le LLM
            generate_rag_answer(ollama_client, question, context)

        except Exception as e:
            print(f"\n‚úó Erreur : {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()