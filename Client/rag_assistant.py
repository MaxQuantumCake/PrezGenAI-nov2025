#!/usr/bin/env python3
"""
Assistant RAG - Combine recherche OpenSearch et g√©n√©ration avec Ollama
Utilise les modules faq_search, pls_search et ollama_client
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer

# Ajouter le dossier Client au path pour les imports
sys.path.insert(0, str(Path(__file__).parent))

# Importer les modules de recherche
import faq_search
import pls_search
from ollama_client import OllamaClient

# Charger les variables d'environnement
PROJECT_ROOT = Path(__file__).parent.parent
env_path = PROJECT_ROOT / '.env'
load_dotenv(env_path)

# Configuration
EMBEDDING_MODEL = os.environ['EMBEDDING_MODEL']
ML_MODEL_ID = os.environ.get('MODEL_ID', '')
OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', 'llama3.2')


# ============================================================================
# FORMATAGE DES R√âSULTATS
# ============================================================================

def format_faq_results_as_context(response):
    """Formate les r√©sultats FAQ en contexte pour le LLM"""
    hits = response["hits"]["hits"]

    if not hits:
        return "Aucun r√©sultat trouv√© dans la FAQ."

    context_parts = []
    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        context_parts.append(
            f"[Document {i} - Pertinence: {score:.2f}]\n"
            f"Question: {source['question']}\n"
            f"R√©ponse: {source['answer']}\n"
        )

    return "\n".join(context_parts)


def format_pls_results_as_context(response):
    """Formate les r√©sultats Pour La Science en contexte pour le LLM"""
    hits = response["hits"]["hits"]

    if not hits:
        return "Aucun r√©sultat trouv√© dans Pour La Science."

    context_parts = []
    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        title = source.get('title', '')
        title_str = f"Titre: {title}\n" if title else ""

        context_parts.append(
            f"[Document {i} - Pertinence: {score:.2f}]\n"
            f"Source: {source['filename']} (Page {source['page']})\n"
            f"{title_str}"
            f"Contenu: {source['text']}\n"
        )

    return "\n".join(context_parts)


def display_faq_results(response):
    """Affiche les r√©sultats FAQ de mani√®re lisible"""
    hits = response["hits"]["hits"]
    total = response["hits"]["total"]["value"]

    print(f"\n{'=' * 70}")
    print(f"üìö R√©sultats FAQ : {total} documents trouv√©s")
    print(f"{'=' * 70}\n")

    if not hits:
        print("Aucun r√©sultat trouv√©.")
        return

    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        print(f"--- Document {i} (score: {score:.4f}) ---")
        print(f"Q: {source['question']}")
        answer = source['answer']
        if len(answer) > 150:
            answer = answer[:150] + "..."
        print(f"R: {answer}")
        if source.get('tags'):
            print(f"Tags: {', '.join(source['tags'])}")
        print()


def display_pls_results(response):
    """Affiche les r√©sultats Pour La Science de mani√®re lisible"""
    hits = response["hits"]["hits"]
    total = response["hits"]["total"]["value"]

    print(f"\n{'=' * 70}")
    print(f"üì∞ R√©sultats Pour La Science : {total} documents trouv√©s")
    print(f"{'=' * 70}\n")

    if not hits:
        print("Aucun r√©sultat trouv√©.")
        return

    for i, hit in enumerate(hits, 1):
        source = hit["_source"]
        score = hit["_score"]

        print(f"--- Document {i} (score: {score:.4f}) ---")
        print(f"Fichier: {source['filename']} - Page {source['page']}")

        if source.get('title'):
            print(f"Titre: {source['title']}")

        text = source['text']
        if len(text) > 150:
            text = text[:150] + "..."
        print(f"Texte: {text}")
        print()


# ============================================================================
# INTERFACE UTILISATEUR
# ============================================================================

def select_corpus():
    """S√©lection du corpus de recherche"""
    print("\nChoisissez le corpus de recherche :")
    print("-" * 70)
    print("1. FAQ CielNet")
    print("2. Pour La Science")
    print("-" * 70)

    while True:
        choice = input("\nVotre choix (1-2) : ").strip()
        if choice == '1':
            print("‚úì Corpus s√©lectionn√© : FAQ CielNet")
            return 'faq'
        elif choice == '2':
            print("‚úì Corpus s√©lectionn√© : Pour La Science")
            return 'pour_la_science'
        else:
            print("Choix invalide. Veuillez entrer 1 ou 2.")


def select_search_mode():
    """S√©lection du mode de recherche"""
    print("\nChoisissez le mode de recherche :")
    print("-" * 70)
    print("1. Mot-cl√© (BM25)")
    print("2. S√©mantique (KNN avec embeddings)")
    print("3. Neural (embeddings OpenSearch)")
    print("4. Hybride (BM25 + Neural)")
    print("-" * 70)

    while True:
        choice = input("\nVotre choix (1-4) : ").strip()
        if choice == '1':
            print("‚úì Mode : Recherche par mot-cl√©")
            return 'keyword'
        elif choice == '2':
            print("‚úì Mode : Recherche s√©mantique")
            return 'semantic'
        elif choice == '3':
            if not ML_MODEL_ID:
                print("‚ö†Ô∏è  MODEL_ID non configur√© - Mode mot-cl√© utilis√© par d√©faut")
                return 'keyword'
            print("‚úì Mode : Recherche neural")
            return 'neural'
        elif choice == '4':
            if not ML_MODEL_ID:
                print("‚ö†Ô∏è  MODEL_ID non configur√© - Mode mot-cl√© utilis√© par d√©faut")
                return 'keyword'
            print("‚úì Mode : Recherche hybride")
            return 'hybrid'
        else:
            print("Choix invalide. Veuillez entrer 1, 2, 3 ou 4.")


def select_llm_model(ollama_client):
    """S√©lection du mod√®le LLM"""
    models = ollama_client.list_models()

    if not models:
        print("‚ö†Ô∏è  Aucun mod√®le Ollama trouv√©, utilisation du mod√®le par d√©faut")
        return ollama_client.model

    model_names = [m.get('name') for m in models]

    print("\nMod√®les Ollama disponibles :")
    print("-" * 70)

    for i, model_name in enumerate(model_names, 1):
        marker = " (actuel)" if model_name == ollama_client.model else ""
        print(f"{i}. {model_name}{marker}")

    print("-" * 70)

    while True:
        choice = input(f"\nVotre choix (1-{len(model_names)}) ou Entr√©e pour garder actuel : ").strip()

        if not choice:
            print(f"‚úì Mod√®le s√©lectionn√© : {ollama_client.model}")
            return ollama_client.model

        if choice.isdigit() and 1 <= int(choice) <= len(model_names):
            selected = model_names[int(choice) - 1]
            print(f"‚úì Mod√®le s√©lectionn√© : {selected}")
            return selected
        else:
            print(f"Choix invalide. Veuillez entrer un nombre entre 1 et {len(model_names)}.")


def perform_search(opensearch_client, embedding_model, corpus_type, search_mode, question, num_results=5):
    """Effectue la recherche selon le corpus et le mode s√©lectionn√©s"""

    if corpus_type == 'faq':
        # D√©terminer l'index
        if search_mode in ['neural', 'hybrid']:
            index_name = faq_search.FAQ_INDEX_NAME_PIPELINE
        elif search_mode == 'semantic':
            index_name = faq_search.FAQ_INDEX_NAME_SEMANTIC
        else:
            index_name = faq_search.FAQ_INDEX_NAME

        # Effectuer la recherche
        if search_mode == 'keyword':
            return faq_search.search_faq_by_keyword(opensearch_client, index_name, question, num_results)
        elif search_mode == 'semantic':
            return faq_search.search_faq_semantic(opensearch_client, embedding_model, question, num_results)
        elif search_mode == 'neural':
            return faq_search.search_faq_neural(opensearch_client, ML_MODEL_ID, question, num_results)
        elif search_mode == 'hybrid':
            return faq_search.search_faq_hybrid(opensearch_client, ML_MODEL_ID, question, num_results)

    else:  # pour_la_science
        # D√©terminer l'index
        if search_mode in ['neural', 'hybrid']:
            index_name = pls_search.PLS_INDEX_NAME_PIPELINE
        elif search_mode == 'semantic':
            index_name = pls_search.PLS_INDEX_NAME_SEMANTIC
        else:
            index_name = pls_search.PLS_INDEX_NAME

        # Effectuer la recherche
        if search_mode == 'keyword':
            return pls_search.search_pls_by_keyword(opensearch_client, index_name, question, num_results)
        elif search_mode == 'semantic':
            return pls_search.search_pls_semantic(opensearch_client, embedding_model, question, num_results)
        elif search_mode == 'neural':
            return pls_search.search_pls_neural(opensearch_client, ML_MODEL_ID, question, num_results)
        elif search_mode == 'hybrid':
            return pls_search.search_pls_hybrid(opensearch_client, ML_MODEL_ID, question, num_results)


def generate_rag_answer(ollama_client, question, context):
    """G√©n√®re une r√©ponse RAG avec Ollama"""
    prompt = f"""Tu es un assistant qui r√©pond aux questions en te basant UNIQUEMENT sur le contexte fourni.

CONTEXTE DOCUMENTAIRE:
{context}

QUESTION: {question}

INSTRUCTIONS:
- R√©ponds √† la question en te basant uniquement sur le contexte fourni
- Si le contexte ne contient pas d'information pertinente pour r√©pondre, dis-le clairement
- Sois pr√©cis, concis et factuel
- Cite les sources quand c'est pertinent (num√©ro de document, page, etc.)

R√âPONSE:"""

    print(f"\n{'=' * 70}")
    print(f"ü§ñ R√©ponse de {ollama_client.model} :")
    print(f"{'=' * 70}\n")

    full_response = ""
    for chunk in ollama_client.generate(prompt, stream=True):
        print(chunk, end='', flush=True)
        full_response += chunk

    print("\n")
    return full_response


# ============================================================================
# FONCTION PRINCIPALE
# ============================================================================

def main():
    """Fonction principale"""
    print("=" * 70)
    print("=== üöÄ Assistant RAG - Recherche Augment√©e par G√©n√©ration ===")
    print("=" * 70)

    # [1/4] Connexion √† OpenSearch
    print("\n[1/4] Connexion √† OpenSearch...")
    try:
        opensearch_client = faq_search.create_opensearch_client()
        info = opensearch_client.info()
        print(f"‚úì Connect√© √† OpenSearch version {info['version']['number']}")
    except Exception as e:
        print(f"‚úó Erreur de connexion √† OpenSearch : {e}")
        return

    # [2/4] Connexion √† Ollama
    print("\n[2/4] Connexion √† Ollama...")
    ollama_client = OllamaClient()

    if not ollama_client.check_connection():
        print("‚úó Impossible de se connecter √† Ollama")
        print("üí° Assurez-vous qu'Ollama est lanc√© : ollama serve")
        return

    print(f"‚úì Connect√© √† Ollama")

    # [3/4] Configuration
    print("\n[3/4] Configuration")
    corpus_type = select_corpus()
    search_mode = select_search_mode()
    llm_model = select_llm_model(ollama_client)
    ollama_client.model = llm_model

    # Charger le mod√®le d'embedding si n√©cessaire
    embedding_model = None
    if search_mode == 'semantic':
        print(f"\n‚è≥ Chargement du mod√®le d'embedding {EMBEDDING_MODEL}...")
        try:
            embedding_model = SentenceTransformer(EMBEDDING_MODEL)
            print("‚úì Mod√®le d'embedding charg√©")
        except Exception as e:
            print(f"‚úó Erreur lors du chargement : {e}")
            return

    # [4/4] Interface de questions-r√©ponses
    print("\n[4/4] Assistant RAG pr√™t")
    print("\n" + "=" * 70)
    print("üí¨ Posez vos questions !")
    print("\nCommandes disponibles :")
    print("  - Tapez votre question pour obtenir une r√©ponse")
    print("  - '/config' pour changer la configuration")
    print("  - '/exit' pour quitter")
    print("-" * 70)

    while True:
        question = input("\n‚ùì Question > ").strip()

        if not question:
            continue

        if question.lower() in ['/exit', '/quit', '/q']:
            print("\nüëã Au revoir!")
            break

        if question.lower() == '/config':
            print("\n" + "=" * 70)
            print("Reconfiguration")
            print("=" * 70)

            corpus_type = select_corpus()
            search_mode = select_search_mode()
            llm_model = select_llm_model(ollama_client)
            ollama_client.model = llm_model

            # Recharger l'embedding model si n√©cessaire
            if search_mode == 'semantic' and embedding_model is None:
                print(f"\n‚è≥ Chargement du mod√®le d'embedding {EMBEDDING_MODEL}...")
                try:
                    embedding_model = SentenceTransformer(EMBEDDING_MODEL)
                    print("‚úì Mod√®le d'embedding charg√©")
                except Exception as e:
                    print(f"‚úó Erreur : {e}")
                    search_mode = 'keyword'
                    print("‚ö†Ô∏è  Retour au mode mot-cl√©")

            continue

        # Effectuer la recherche
        print(f"\nüîç Recherche en cours ({search_mode})...")

        try:
            response = perform_search(
                opensearch_client,
                embedding_model,
                corpus_type,
                search_mode,
                question
            )

            # Afficher les r√©sultats de recherche
            if corpus_type == 'faq':
                display_faq_results(response)
                context = format_faq_results_as_context(response)
            else:
                display_pls_results(response)
                context = format_pls_results_as_context(response)

            # G√©n√©rer la r√©ponse avec le LLM
            generate_rag_answer(ollama_client, question, context)

        except Exception as e:
            print(f"\n‚úó Erreur : {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()